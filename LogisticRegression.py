# -*- coding: utf-8 -*-

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pq7ihmlnTyXviyWsxJEh9s3yq7ScaYmK
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
# Read a dummy dataset 
np.random.seed(12)
num_observations = 5000
x1 = np.random.multivariate_normal([0,0],[[1,0.75],[0.75,1]],num_observations)
x2 = np.random.multivariate_normal([1,4],[[1,0.75],[0.75,1]],num_observations)
features = np.vstack((x1,x2)).astype(np.float32)
labels = np.hstack((np.full(num_observations,-1),np.ones(num_observations)))
#features in tabular format
df = pd.DataFrame(features, columns=['A','B'])
#concatenate labels with feautures data
df['labels'] = pd.Series(labels, index=df.index)

data = df.loc[:, ['A', 'B']]
target = df.loc[:, 'labels']
#split the data randomly into test and train data
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.20, random_state=42)
#function to calculate linear 
def calculate_linear(features, weights):
        return np.dot(features, weights)
# function to calculate loss function
def loss_function(features, labels, weights):
   # return (np.log(1+np.exp(-np.dot(calculate_linear(features, weights), labels)))).mean()
    return (-labels * np.log(calculate_linear(features, weights)) - (1 - labels) * np.log(1 - calculate_linear(features, weights))).mean()
#function to calculate gradient
def calc_gradient(features, labels, weights):
    nume = np.dot(labels, features)
    denm = np.exp(np.dot(labels, calculate_linear(features, weights)))
    return (-nume/(1+denm))
#function to calculate Logistic regression
def logistic_regression(features, labels, num_steps, learning_rate, intercept=False):
    if (intercept):
        inter = np.ones(features.shape[0])
        features['intercept'] = pd.Series(inter, index=features.index)
    weights = np.zeros(features.shape[1])
    for step in range(num_steps):
        gradient = calc_gradient(features, labels, weights)
        avg_gradient = np.mean(gradient, axis=0)
        weights -= learning_rate*avg_gradient
        if (step%1000 == 0):
           print(loss_function(features, labels, weights))
    return weights
#function to calculate the sigmoid of linear expression
def sigmoid(features, weights, intercept=False):
    if (intercept):
        inter = np.ones(features.shape[0])
        features['intercept'] = pd.Series(inter, index=features.index)
    return (1/(1,np.exp(-np.dot(features, weights))))
#To calculate sigmoidal integrity 
def convert_labels(mylist):
    for i in range(len(mylist)):
        if (mylist[i] >=0.5):
            mylist[i] = 1
        else:
            mylist[i] = -1
    return mylist
#without intercept
weights_train = logistic_regression(X_train, y_train, 30000, 0.01)
print("Train Weights without intercept: ",weights_train)
weights_test = logistic_regression(X_test, y_test, 30000, 0.01)
print("Test Weights without intercept: ",weights_test)
train_predictions = sigmoid(X_train, weights_train)

train_predictions = convert_labels(train_predictions)

test_predictions = sigmoid(X_test, weights_test)

test_predictions =convert_labels(test_predictions)
#sklearn logistic regression without intercept
clf = LogisticRegression(fit_intercept=False).fit(X_train, y_train)

y_pred = clf.predict(X_test)

print("Train accuracy without intercept: ", accuracy_score(y_train, train_predictions))
print("Test accuracy without intercept : ", accuracy_score(y_test, test_predictions))
print("Test accuracy Sklearn without intercept: ", accuracy_score(y_test, y_pred))
print("Confusion matrix without intercept: \n", confusion_matrix(y_test, y_pred))
print("Classification report with intercept: \n", classification_report(y_test, y_pred, target_names=['1','-1']))


#with Intercept
weights_train_with_intercept = logistic_regression(X_train, y_train, 30000, 0.01, True)
print("Train Weights with intercept: ",weights_test_with_intercept)
weights_test_with_intercept = logistic_regression(X_test, y_test, 30000, 0.01, True)
print("Test Weights with intercept: ",weights_test_with_intercept)
train_predictions_intercept = sigmoid(X_train, weights_train_with_intercept, True)

train_predictions_intercept = convert_labels(train_predictions)

test_predictions_intercept = sigmoid(X_test, weights_test_with_intercept, True)

test_predictions_intercept =convert_labels(test_predictions)
#sklearn logistic regrssion with intercept
clf_intercept = LogisticRegression(fit_intercept=True).fit(X_train, y_train)

y_pred_intercept = clf_intercept.predict(X_test)

print("Train accuracy with intercept: ", accuracy_score(y_train, train_predictions_intercept))
print("Test accuracy with intercept : ", accuracy_score(y_test, test_predictions_intercept))
print("Test accuracy Sklearn with Intercept : ", accuracy_score(y_test, y_pred_intercept))
print("Confusion matrix with intercept: \n", confusion_matrix(y_test, y_pred_intercept))
print("Classification report with intercept: \n", classification_report(y_test, y_pred_intercept, target_names=['1','-1']))
print("Weights with intercept: ",weights_test_with_intercept)

# %matplotlib inline
plt.figure(figsize = (12,8))
plt.scatter(features[:,0], features[:,1], c = labels, alpha = 0.4)
plt.xlabel("A")
plt.ylabel("B")
a = weights_with_intercept[0]/weights_with_intercept[1]
xx = np.linspace(-5,5)
yy = a* xx- weights_with_intercept[2]/weights_with_intercept[1]
plt.plot(xx,yy,'k-',color='red')
a = weights[0]/weights[1]
yy = a * xx
plt.plot(xx,yy,'k-',color='green')
